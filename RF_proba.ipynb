{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def import_data(file):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 34.56 MB\n",
      "Memory usage after optimization is: 9.26 MB\n",
      "Decreased by 73.2%\n"
     ]
    }
   ],
   "source": [
    "df = import_data(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.79      0.82      0.81      1079\n",
      "           4       0.11      0.11      0.11        99\n",
      "           5       0.73      0.77      0.75      1344\n",
      "           6       0.67      0.68      0.68       386\n",
      "           7       0.63      0.63      0.63      1774\n",
      "           8       0.67      0.67      0.67      3638\n",
      "           9       0.56      0.59      0.57      2784\n",
      "          12       0.09      0.04      0.05        80\n",
      "          15       0.40      0.33      0.36       295\n",
      "          18       0.29      0.27      0.28       162\n",
      "          19       0.27      0.18      0.22       116\n",
      "          20       0.51      0.46      0.48       204\n",
      "          21       0.57      0.50      0.53       211\n",
      "          22       0.33      0.27      0.30       250\n",
      "          23       0.37      0.39      0.38        36\n",
      "          24       0.52      0.54      0.53       781\n",
      "          25       0.56      0.63      0.59      1147\n",
      "          26       0.40      0.32      0.35       152\n",
      "          27       0.52      0.46      0.49       261\n",
      "          28       0.35      0.26      0.30       156\n",
      "          29       0.18      0.09      0.12       128\n",
      "          30       0.37      0.31      0.34       326\n",
      "          31       0.61      0.52      0.56       187\n",
      "          32       0.61      0.68      0.64       619\n",
      "          33       0.53      0.49      0.51       370\n",
      "          34       0.48      0.42      0.44       231\n",
      "          35       0.47      0.39      0.42       627\n",
      "          36       0.50      0.55      0.53       897\n",
      "          37       0.57      0.53      0.55       852\n",
      "          38       0.48      0.36      0.41       869\n",
      "          39       0.47      0.68      0.56      2928\n",
      "          40       0.74      0.79      0.76      1813\n",
      "          41       0.08      0.01      0.02       190\n",
      "          42       0.34      0.19      0.24       537\n",
      "          43       0.19      0.03      0.06       267\n",
      "          44       0.26      0.03      0.05       352\n",
      "         999       0.85      0.78      0.81      2555\n",
      "\n",
      "   micro avg       0.60      0.60      0.60     28703\n",
      "   macro avg       0.46      0.43      0.44     28703\n",
      "weighted avg       0.59      0.60      0.59     28703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# 아래 코드\n",
    "df_d_sum = df.groupby(['VisitNumber', 'DepartmentDescription'], as_index=False)['ScanCount'].agg('sum').sort_values(['VisitNumber', 'ScanCount', 'DepartmentDescription'], ascending=[1, 0, 1])\n",
    "df_d_sum['abs_sum'] = df_d_sum.assign(abs=df_d_sum['ScanCount'].abs()).groupby(['VisitNumber'])['abs'].transform('sum')\n",
    "\n",
    "# change 'abs_sum' for div\n",
    "criteria = df_d_sum['abs_sum'] == 0\n",
    "df_d_sum.loc[criteria, 'abs_sum'] = 999\n",
    "\n",
    "# create ratio\n",
    "df_d_sum['ratio'] = df_d_sum['ScanCount'] / df_d_sum['abs_sum']\n",
    "\n",
    "# abs_sum 원복\n",
    "criteria = df_d_sum['abs_sum'] == 999\n",
    "df_d_sum.loc[criteria, 'abs_sum'] = 0\n",
    "\n",
    "# Dept Na 였던 VisitNumber append\n",
    "diff = set(df.VisitNumber) - set(df_d_sum.VisitNumber)\n",
    "df_d_sum = df_d_sum.append(pd.DataFrame({'VisitNumber': list(diff)},)).fillna({'DepartmentDescription': 'Na',\n",
    "                                                                          'ScanCount': 0,\n",
    "                                                                         'ratio':0 })\n",
    "# create pivot table\n",
    "df_pivot = df_d_sum.pivot(index='VisitNumber', columns='DepartmentDescription', values='ratio').fillna(0)\n",
    "df_pivot = pd.merge(df_pivot, df[['VisitNumber', 'TripType']].drop_duplicates(), on='VisitNumber')\n",
    "\n",
    "# make training data\n",
    "X = df_pivot.iloc[:, :-1]\n",
    "y = df_pivot.iloc[:, -1]\n",
    "\n",
    "X_t, X_te, y_t, y_te = train_test_split(X, y, test_size = 0.3, random_state=99)\n",
    "\n",
    "# training\n",
    "\n",
    "mod2 = RandomForestClassifier(n_estimators=200, bootstrap=False, min_samples_leaf=1, min_samples_split=3,\\\n",
    "                             criterion='gini').fit(X_t, y_t)\n",
    "\n",
    "print(classification_report(y_te, mod2.predict(X_te)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'rf_model.sav'\n",
    "pickle.dump(mod2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
